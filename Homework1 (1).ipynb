{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f5a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "from torch import nn, optim  \n",
    "import torch.nn.functional as F \n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddba282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "# datasets\n",
    "trainset = datasets.FashionMNIST('./data',\n",
    " download=True,\n",
    " train=True,\n",
    " transform=transform)\n",
    "\n",
    "testset =datasets.FashionMNIST('./data',\n",
    " download=True,\n",
    " train=False,\n",
    " transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba21520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10,\n",
    " shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10,\n",
    " shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c722521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c0b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3080dfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(dataloader, model, criterion, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ba244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += criterion(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6c772fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.279510  [    0/60000]\n",
      "loss: 2.252418  [ 1000/60000]\n",
      "loss: 2.219891  [ 2000/60000]\n",
      "loss: 2.091026  [ 3000/60000]\n",
      "loss: 2.158930  [ 4000/60000]\n",
      "loss: 1.976520  [ 5000/60000]\n",
      "loss: 1.886012  [ 6000/60000]\n",
      "loss: 1.834839  [ 7000/60000]\n",
      "loss: 1.436797  [ 8000/60000]\n",
      "loss: 1.314520  [ 9000/60000]\n",
      "loss: 1.260857  [10000/60000]\n",
      "loss: 1.836877  [11000/60000]\n",
      "loss: 1.581339  [12000/60000]\n",
      "loss: 0.946002  [13000/60000]\n",
      "loss: 1.043094  [14000/60000]\n",
      "loss: 1.511350  [15000/60000]\n",
      "loss: 1.103208  [16000/60000]\n",
      "loss: 1.138939  [17000/60000]\n",
      "loss: 1.060771  [18000/60000]\n",
      "loss: 1.183682  [19000/60000]\n",
      "loss: 1.282795  [20000/60000]\n",
      "loss: 0.757225  [21000/60000]\n",
      "loss: 0.920832  [22000/60000]\n",
      "loss: 0.908629  [23000/60000]\n",
      "loss: 0.980191  [24000/60000]\n",
      "loss: 0.839678  [25000/60000]\n",
      "loss: 1.027668  [26000/60000]\n",
      "loss: 0.676509  [27000/60000]\n",
      "loss: 0.577926  [28000/60000]\n",
      "loss: 0.880919  [29000/60000]\n",
      "loss: 0.666079  [30000/60000]\n",
      "loss: 1.042536  [31000/60000]\n",
      "loss: 1.228166  [32000/60000]\n",
      "loss: 0.620550  [33000/60000]\n",
      "loss: 0.476267  [34000/60000]\n",
      "loss: 0.848951  [35000/60000]\n",
      "loss: 0.767447  [36000/60000]\n",
      "loss: 0.678555  [37000/60000]\n",
      "loss: 0.823823  [38000/60000]\n",
      "loss: 0.459977  [39000/60000]\n",
      "loss: 0.627062  [40000/60000]\n",
      "loss: 0.601859  [41000/60000]\n",
      "loss: 0.730820  [42000/60000]\n",
      "loss: 0.391162  [43000/60000]\n",
      "loss: 0.464545  [44000/60000]\n",
      "loss: 0.314320  [45000/60000]\n",
      "loss: 0.402113  [46000/60000]\n",
      "loss: 1.006672  [47000/60000]\n",
      "loss: 0.493632  [48000/60000]\n",
      "loss: 0.692642  [49000/60000]\n",
      "loss: 0.424170  [50000/60000]\n",
      "loss: 0.917848  [51000/60000]\n",
      "loss: 0.650633  [52000/60000]\n",
      "loss: 0.990129  [53000/60000]\n",
      "loss: 0.630813  [54000/60000]\n",
      "loss: 0.727861  [55000/60000]\n",
      "loss: 0.425115  [56000/60000]\n",
      "loss: 0.514509  [57000/60000]\n",
      "loss: 1.162544  [58000/60000]\n",
      "loss: 0.656784  [59000/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.653542 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.659520  [    0/60000]\n",
      "loss: 1.106257  [ 1000/60000]\n",
      "loss: 0.431149  [ 2000/60000]\n",
      "loss: 0.587512  [ 3000/60000]\n",
      "loss: 0.936896  [ 4000/60000]\n",
      "loss: 0.324489  [ 5000/60000]\n",
      "loss: 0.929354  [ 6000/60000]\n",
      "loss: 0.879677  [ 7000/60000]\n",
      "loss: 0.937195  [ 8000/60000]\n",
      "loss: 0.919266  [ 9000/60000]\n",
      "loss: 0.154340  [10000/60000]\n",
      "loss: 0.521609  [11000/60000]\n",
      "loss: 0.602753  [12000/60000]\n",
      "loss: 0.928726  [13000/60000]\n",
      "loss: 0.374061  [14000/60000]\n",
      "loss: 0.643063  [15000/60000]\n",
      "loss: 0.361458  [16000/60000]\n",
      "loss: 0.679409  [17000/60000]\n",
      "loss: 0.577346  [18000/60000]\n",
      "loss: 0.582339  [19000/60000]\n",
      "loss: 0.545794  [20000/60000]\n",
      "loss: 0.323082  [21000/60000]\n",
      "loss: 0.317590  [22000/60000]\n",
      "loss: 0.759322  [23000/60000]\n",
      "loss: 0.543131  [24000/60000]\n",
      "loss: 0.621708  [25000/60000]\n",
      "loss: 0.207546  [26000/60000]\n",
      "loss: 0.670733  [27000/60000]\n",
      "loss: 0.405761  [28000/60000]\n",
      "loss: 0.389803  [29000/60000]\n",
      "loss: 0.728574  [30000/60000]\n",
      "loss: 1.019492  [31000/60000]\n",
      "loss: 0.649330  [32000/60000]\n",
      "loss: 0.246138  [33000/60000]\n",
      "loss: 0.351645  [34000/60000]\n",
      "loss: 0.701324  [35000/60000]\n",
      "loss: 0.434912  [36000/60000]\n",
      "loss: 0.093458  [37000/60000]\n",
      "loss: 0.405279  [38000/60000]\n",
      "loss: 0.773516  [39000/60000]\n",
      "loss: 0.864532  [40000/60000]\n",
      "loss: 0.446142  [41000/60000]\n",
      "loss: 0.473959  [42000/60000]\n",
      "loss: 1.162054  [43000/60000]\n",
      "loss: 0.246148  [44000/60000]\n",
      "loss: 0.345336  [45000/60000]\n",
      "loss: 0.420345  [46000/60000]\n",
      "loss: 0.718929  [47000/60000]\n",
      "loss: 0.290925  [48000/60000]\n",
      "loss: 0.383806  [49000/60000]\n",
      "loss: 0.776919  [50000/60000]\n",
      "loss: 0.224709  [51000/60000]\n",
      "loss: 0.623362  [52000/60000]\n",
      "loss: 0.447628  [53000/60000]\n",
      "loss: 1.228938  [54000/60000]\n",
      "loss: 0.335006  [55000/60000]\n",
      "loss: 0.321555  [56000/60000]\n",
      "loss: 0.805600  [57000/60000]\n",
      "loss: 0.475312  [58000/60000]\n",
      "loss: 0.606851  [59000/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.552580 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(trainloader, model, criterion, optimizer)\n",
    "    test(testloader, model, criterion)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c480fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork1, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2e44e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model1 = NeuralNetwork1().to(device)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61fcae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(dataloader, model1, criterion, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model1.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model1(X)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9b4ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model1, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model1.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model1(X)\n",
    "            test_loss += criterion(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c7cb75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.350770  [    0/60000]\n",
      "loss: 2.336334  [ 1000/60000]\n",
      "loss: 2.370698  [ 2000/60000]\n",
      "loss: 2.324197  [ 3000/60000]\n",
      "loss: 2.304630  [ 4000/60000]\n",
      "loss: 2.430843  [ 5000/60000]\n",
      "loss: 2.353748  [ 6000/60000]\n",
      "loss: 2.306098  [ 7000/60000]\n",
      "loss: 2.437776  [ 8000/60000]\n",
      "loss: 2.294260  [ 9000/60000]\n",
      "loss: 2.345560  [10000/60000]\n",
      "loss: 2.333411  [11000/60000]\n",
      "loss: 2.296947  [12000/60000]\n",
      "loss: 2.218808  [13000/60000]\n",
      "loss: 2.295458  [14000/60000]\n",
      "loss: 2.367568  [15000/60000]\n",
      "loss: 2.363127  [16000/60000]\n",
      "loss: 2.338758  [17000/60000]\n",
      "loss: 2.331880  [18000/60000]\n",
      "loss: 2.453960  [19000/60000]\n",
      "loss: 2.358605  [20000/60000]\n",
      "loss: 2.358385  [21000/60000]\n",
      "loss: 2.357728  [22000/60000]\n",
      "loss: 2.299108  [23000/60000]\n",
      "loss: 2.340860  [24000/60000]\n",
      "loss: 2.389213  [25000/60000]\n",
      "loss: 2.362401  [26000/60000]\n",
      "loss: 2.287502  [27000/60000]\n",
      "loss: 2.361770  [28000/60000]\n",
      "loss: 2.334127  [29000/60000]\n",
      "loss: 2.385093  [30000/60000]\n",
      "loss: 2.297667  [31000/60000]\n",
      "loss: 2.384673  [32000/60000]\n",
      "loss: 2.276328  [33000/60000]\n",
      "loss: 2.310604  [34000/60000]\n",
      "loss: 2.336835  [35000/60000]\n",
      "loss: 2.345575  [36000/60000]\n",
      "loss: 2.375875  [37000/60000]\n",
      "loss: 2.303330  [38000/60000]\n",
      "loss: 2.294614  [39000/60000]\n",
      "loss: 2.314684  [40000/60000]\n",
      "loss: 2.178773  [41000/60000]\n",
      "loss: 2.364753  [42000/60000]\n",
      "loss: 2.464733  [43000/60000]\n",
      "loss: 2.297407  [44000/60000]\n",
      "loss: 2.350253  [45000/60000]\n",
      "loss: 2.270325  [46000/60000]\n",
      "loss: 2.277465  [47000/60000]\n",
      "loss: 2.353771  [48000/60000]\n",
      "loss: 2.284485  [49000/60000]\n",
      "loss: 2.365247  [50000/60000]\n",
      "loss: 2.284330  [51000/60000]\n",
      "loss: 2.328958  [52000/60000]\n",
      "loss: 2.370087  [53000/60000]\n",
      "loss: 2.220270  [54000/60000]\n",
      "loss: 2.234217  [55000/60000]\n",
      "loss: 2.475166  [56000/60000]\n",
      "loss: 2.253181  [57000/60000]\n",
      "loss: 2.355341  [58000/60000]\n",
      "loss: 2.273553  [59000/60000]\n",
      "Test Error: \n",
      " Accuracy: 8.3%, Avg loss: 2.341681 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.278363  [    0/60000]\n",
      "loss: 2.392655  [ 1000/60000]\n",
      "loss: 2.344781  [ 2000/60000]\n",
      "loss: 2.366065  [ 3000/60000]\n",
      "loss: 2.434889  [ 4000/60000]\n",
      "loss: 2.385218  [ 5000/60000]\n",
      "loss: 2.335954  [ 6000/60000]\n",
      "loss: 2.415758  [ 7000/60000]\n",
      "loss: 2.382077  [ 8000/60000]\n",
      "loss: 2.295574  [ 9000/60000]\n",
      "loss: 2.304361  [10000/60000]\n",
      "loss: 2.365688  [11000/60000]\n",
      "loss: 2.387380  [12000/60000]\n",
      "loss: 2.375854  [13000/60000]\n",
      "loss: 2.267300  [14000/60000]\n",
      "loss: 2.391078  [15000/60000]\n",
      "loss: 2.369981  [16000/60000]\n",
      "loss: 2.346131  [17000/60000]\n",
      "loss: 2.223716  [18000/60000]\n",
      "loss: 2.289654  [19000/60000]\n",
      "loss: 2.417765  [20000/60000]\n",
      "loss: 2.428751  [21000/60000]\n",
      "loss: 2.396574  [22000/60000]\n",
      "loss: 2.352453  [23000/60000]\n",
      "loss: 2.248034  [24000/60000]\n",
      "loss: 2.350928  [25000/60000]\n",
      "loss: 2.274361  [26000/60000]\n",
      "loss: 2.226541  [27000/60000]\n",
      "loss: 2.302694  [28000/60000]\n",
      "loss: 2.335552  [29000/60000]\n",
      "loss: 2.199273  [30000/60000]\n",
      "loss: 2.315116  [31000/60000]\n",
      "loss: 2.350458  [32000/60000]\n",
      "loss: 2.335389  [33000/60000]\n",
      "loss: 2.278241  [34000/60000]\n",
      "loss: 2.392850  [35000/60000]\n",
      "loss: 2.311767  [36000/60000]\n",
      "loss: 2.413850  [37000/60000]\n",
      "loss: 2.391258  [38000/60000]\n",
      "loss: 2.351764  [39000/60000]\n",
      "loss: 2.420566  [40000/60000]\n",
      "loss: 2.361745  [41000/60000]\n",
      "loss: 2.312490  [42000/60000]\n",
      "loss: 2.198406  [43000/60000]\n",
      "loss: 2.439849  [44000/60000]\n",
      "loss: 2.347388  [45000/60000]\n",
      "loss: 2.309755  [46000/60000]\n",
      "loss: 2.297557  [47000/60000]\n",
      "loss: 2.318681  [48000/60000]\n",
      "loss: 2.422484  [49000/60000]\n",
      "loss: 2.314938  [50000/60000]\n",
      "loss: 2.317998  [51000/60000]\n",
      "loss: 2.379157  [52000/60000]\n",
      "loss: 2.384135  [53000/60000]\n",
      "loss: 2.300294  [54000/60000]\n",
      "loss: 2.203053  [55000/60000]\n",
      "loss: 2.342861  [56000/60000]\n",
      "loss: 2.379438  [57000/60000]\n",
      "loss: 2.395055  [58000/60000]\n",
      "loss: 2.434535  [59000/60000]\n",
      "Test Error: \n",
      " Accuracy: 8.3%, Avg loss: 2.341681 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(trainloader, model1, criterion, optimizer)\n",
    "    test(testloader, model1, criterion)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd832e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
